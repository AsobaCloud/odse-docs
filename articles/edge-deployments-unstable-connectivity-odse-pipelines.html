<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Our Rural Sites Kept Losing Data — Here's How We Fixed the Pipeline - ODS-E Blog</title>
  <meta name="description" content="How we stopped blaming inverters for data gaps that were actually caused by spotty cellular connectivity — and the pipeline patterns that made our rural sites reliable.">
  <link rel="icon" type="image/x-icon" href="../favicon.ico">
  <link rel="icon" type="image/png" sizes="32x32" href="../Artboard-7.png">
  <link rel="apple-touch-icon" sizes="180x180" href="../Artboard-7.png">
  <link rel="stylesheet" href="../includes/common.css">
  <style>
    :root { --primary-black:#000; --primary-blue:#455BF1; --text-dark:#333; --text-light:#4D4D4D; --white:#FFF; --border-grey:#E0E0E0; --neutral-grey:#F4F4F4; --accent-red:#E20419; }
    * { margin:0; padding:0; box-sizing:border-box; }
    body { font-family:'DM Sans',Helvetica,Arial,sans-serif; background:var(--white); line-height:1.75; color:var(--text-dark); }
    .header-bar { background:var(--primary-black); position:fixed; top:0; left:0; width:100%; height:70px; z-index:100; display:flex; align-items:center; justify-content:space-between; padding:0 90px; }
    .branding { display:flex; align-items:center; }
    .branding img { width:50px; height:50px; object-fit:contain; }
    .site-title { color:var(--white); font-size:20px; font-weight:700; margin-left:15px; white-space:nowrap; }
    .top-links { display:flex; gap:15px; }
    .top-links a { color:var(--white); text-decoration:none; font:normal 12px/16px 'DM Sans',Helvetica,Arial,sans-serif; padding:5px; }
    .article-container { max-width:920px; margin:0 auto; padding:120px 40px 80px; }
    .article-header { margin-bottom:48px; text-align:center; border-bottom:1px solid var(--border-grey); padding-bottom:28px; }
    .article-type { display:inline-block; padding:8px 16px; background:var(--accent-red); color:var(--white); font-size:.8rem; font-weight:600; border-radius:20px; text-transform:uppercase; letter-spacing:.5px; margin-bottom:20px; }
    .article-title { font-size:2.6rem; font-weight:700; margin-bottom:14px; line-height:1.2; }
    .article-excerpt { font-size:1.15rem; color:var(--text-light); margin-bottom:16px; line-height:1.5; }
    .article-meta { font-size:.98rem; color:var(--primary-blue); font-weight:600; }
    .article-content h2 { font-size:1.8rem; margin:36px 0 14px; line-height:1.3; }
    .article-content h3 { font-size:1.3rem; margin:24px 0 10px; line-height:1.3; }
    .article-content p { margin-bottom:16px; }
    .article-content ul { margin:0 0 16px 22px; }
    .article-content li { margin-bottom:8px; }
    pre { background:var(--neutral-grey); border-radius:8px; padding:16px; overflow-x:auto; margin:14px 0 20px; line-height:1.5; }
    code { font-family:Menlo,Monaco,Consolas,'Courier New',monospace; font-size:.95em; }
    .callout { border:1px solid var(--border-grey); border-left:4px solid var(--primary-blue); border-radius:8px; padding:16px; margin:22px 0; background:#fafbff; }
    .back-link { display:inline-block; margin-top:24px; color:var(--primary-blue); font-weight:600; text-decoration:none; }
    @media (max-width:900px) { .header-bar{padding:0 16px;} .site-title{font-size:18px;} .article-container{padding:110px 20px 56px;} .article-title{font-size:2rem;} }
  </style>
</head>
<body>
  <header class="header-bar">
    <div class="branding">
      <span class="site-title">Open Data Schema for Energy</span>
    </div>
    <div class="top-links">
      <a href="https://opendataschema.energy/blog.html">Blog</a>
      <a href="https://github.com/AsobaCloud/odse">GitHub</a>
      <a href="https://asoba.co/contact-us">Contact</a>
    </div>
  </header>

  <main class="article-container">
    <header class="article-header">
      <span class="article-type">Edge Operations</span>
      <h1 class="article-title">Our Rural Sites Kept Losing Data — Here's How We Fixed the Pipeline</h1>
      <p class="article-excerpt">We spent months blaming inverters for data gaps that were actually caused by spotty cellular connectivity. Here's the pipeline redesign that made our rural and peri-urban sites reliable.</p>
    </header>

    <article class="article-content">
      <p>We manage a portfolio that includes several rural and peri-urban solar sites in the Eastern Cape and Limpopo. Nice installations, solid inverter hardware, decent generation profiles. But for the first six months of operation, the data told a different story. Our completeness dashboard showed regular gaps — some sites dipping to 60-70% on certain days — and the ops team kept opening tickets against the inverters. "Site X underperforming again." "Check the string faults at Site Y."</p>
      <p>Except there were no string faults. The inverters were generating fine. We were just losing the data between the site and the cloud. Spotty cellular connectivity, gateway reboots, upload timeouts — the pipeline assumed a reliable network, and when it didn't get one, it silently dropped records. Every "performance anomaly" in our analytics was actually a communications gap wearing a disguise.</p>
      <p>Once we understood the real problem, we rebuilt the pipeline around a simple principle: normalize and validate at the edge, queue durably, and sync when you can. Here's what that looks like.</p>

      <h2>The Pipeline We Ended Up With</h2>
      <pre><code>collect locally -> durable queue -> ODSE transform -> schema validation -> batch sync -> semantic validation -> warehouse</code></pre>
      <p>The key insight is that data quality controls live close to the source — on the gateway itself or on a local edge device. You don't wait for the data to reach the cloud before checking if it's well-formed. By the time it gets there (if it gets there on the first try), it should already be structurally valid. Cloud-side, you run the heavier semantic checks that need portfolio context — things like "is this generation value plausible given the site's installed capacity and today's irradiance?"</p>

      <h2>What We Changed at the Edge</h2>
      <p>The biggest shift was making local persistence durable. Our original setup wrote transformed records to a temporary directory and uploaded them immediately. If the upload failed, the records were gone. Classic fire-and-forget architecture that only works when the network works.</p>
      <p>We switched to a durable local queue — nothing exotic, just SQLite on the gateway — that holds both the raw payload and the ODSE-transformed record. Every record gets an idempotent key derived from site ID, timestamp, and source hash. That key is what makes replay safe: if the same record arrives twice, the cloud side can deduplicate without worrying about double-counting.</p>
      <p>We also moved timestamp normalization to the edge. Previously, the cloud pipeline was handling timezone conversion, which meant any upload delay introduced ambiguity about which timezone the data was in. Now everything leaves the gateway as timezone-explicit ISO format. No guesswork downstream.</p>

      <h2>The Monday Morning Mystery</h2>
      <p>Here's a concrete example that made us rethink how we monitor these sites. One site in the portfolio was showing 40% completeness every Monday. Tuesdays through Sundays were fine — 95%+ — but Mondays were a disaster. The ops team had it flagged as a recurring inverter fault.</p>
      <p>We dug in and found that the cellular gateway at that site was rebooting on a weekly cron job — some leftover from the ISP's default configuration. The reboot happened at midnight Sunday, the gateway took about 90 minutes to reconnect, and the upload queue wasn't durable. Every record collected between midnight and ~1:30am was lost. But because the generation during those hours was near zero (it's nighttime), the energy totals looked almost normal. The completeness gap was invisible unless you looked at interval-level data.</p>
      <p>After we made the queue durable and the sync idempotent, the gateway could reboot whenever it wanted. Records would queue up locally and sync on reconnection. Monday completeness went from 40% to 98%.</p>

      <h2>Validation in Two Stages</h2>
      <h3>At the edge</h3>
      <p>We run the ODSE transform plus schema validation before a record enters the durable queue. If the record is malformed — null energy fields, missing timestamps, impossible values — it gets rejected right there. This sounds obvious, but our old pipeline would happily ship garbage to the cloud and then fail during analytics. By catching structural errors at the edge, we stopped polluting the backlog with records that could never be valid.</p>

      <h3>At sync</h3>
      <p>When batches arrive cloud-side, we run semantic validation. This is where you check things that require portfolio context: is the reported generation plausible for this site's capacity? Does the interval pattern match what we expect for this source? Have we seen a gap in the sequence that suggests missing data rather than zero generation? Schema validation tells you the record is well-formed. Semantic validation tells you it makes sense.</p>

      <h2>Making Replay Safe</h2>
      <p>The other thing that burned us early on was duplicate records. When the network is flaky, uploads retry. If your sync path isn't idempotent, retries create duplicates, and duplicates inflate your energy totals. We had one site that appeared to be generating 15% above nameplate capacity for a week. Turned out it was the same afternoon's data uploaded three times.</p>
      <p>The fix is straightforward: stable record keys and deduplication at the cloud boundary.</p>
      <pre><code>if seen(record_id):
    skip()
else:
    persist(record)

recompute_completeness(site_id, date)</code></pre>
      <p>After every sync batch, we recompute completeness for the affected site and date range. This catches both newly filled gaps and any intervals that are still missing.</p>

      <h2>The Metrics That Actually Matter</h2>
      <p>We used to track uptime and generation totals. Those are fine, but they don't help you distinguish between "the inverter is down" and "the network is down." Now we track four things:</p>
      <ul>
        <li><strong>Queue depth and age at each gateway</strong> — if records are piling up, the network is the problem, not the asset.</li>
        <li><strong>Transform and validation success rate by source</strong> — tells you whether the OEM export format has changed or degraded.</li>
        <li><strong>Sync lag</strong> (event time vs arrival time) — the gap between when data was generated and when it reached the warehouse.</li>
        <li><strong>Post-reconciliation completeness by site</strong> — the only metric that tells you the full truth after replays have landed.</li>
      </ul>
      <p>The combination of sync lag and post-reconciliation completeness is what finally let our ops team stop blaming inverters for network problems. When sync lag spikes but completeness recovers after replay, it's a comms issue. When completeness stays low even after replay, something's actually wrong at the site.</p>

      <h2>Mistakes We Made (So You Don't Have To)</h2>
      <p>Our original pipeline ran normalization only in the cloud. When the network dropped, raw records were lost before they ever got transformed. Moving the ODSE transform to the edge was the single most impactful change we made.</p>
      <p>We also spent too long treating delayed data as a fault state. Our alerting system would fire "missing data" alerts for sites that were simply queuing records locally during a connectivity window. We learned to separate "data not yet arrived" from "data will never arrive" — the first is a sync delay, the second is a real incident. The durable queue and replay mechanism made this distinction possible.</p>
      <p>And timezone drift — don't ignore it. One of our gateways had its system clock drifting by a few minutes each week because NTP wasn't configured. Small drift, but enough to shift intervals across boundaries and break completeness calculations. Pin your timestamps at transform time and validate the clock source.</p>

      <div class="callout">
        <strong>The lesson we keep coming back to:</strong>
        In unstable connectivity, deterministic replay behavior is as important as transform correctness. If you can't replay safely, you can't trust any historical reconstruction — and every data gap becomes permanent.
      </div>

      <h2>Where We Are Now</h2>
      <p>Our rural sites went from being the "problem children" of the portfolio to some of the most reliable data sources we have. Not because the connectivity improved — it didn't — but because the pipeline stopped pretending the connectivity was something it wasn't. We design for the network conditions we actually have, not the ones we wish we had. Durable queues, idempotent sync, edge validation, and honest completeness metrics. That's the entire playbook.</p>

      <p><a href="/docs/transforms/overview">Transforms</a> | <a href="/docs/validation/schema-validation">Schema Validation</a> | <a href="/docs/validation/semantic-validation">Semantic Validation</a></p>
      <a class="back-link" href="/blog.html">&larr; Back to Blog</a>
    </article>
  </main>
</body>
</html>
